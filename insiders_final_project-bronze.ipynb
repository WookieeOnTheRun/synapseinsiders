{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# verify loaded packages\r\n",
        "import pkg_resources\r\n",
        "\r\n",
        "for p in pkg_resources.working_set :\r\n",
        "\r\n",
        "    print( p )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# resources :\r\n",
        "#   https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files\r\n",
        "#   https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-what-is-delta-lake\r\n",
        "#   https://learn.microsoft.com/en-us/azure/synapse-analytics/how-to-analyze-complex-schema\r\n",
        "\r\n",
        "\r\n",
        "# import necessary packages and libraries\r\n",
        "import json, datetime, time\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "from delta import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define necessary connections to storage ( source and destination )\r\n",
        "# abfss because my storage account has HNS enabled\r\n",
        "adlsAcct = \"<storage account url>\"\r\n",
        "adlsSas = \"<sas token>\"\r\n",
        "adlsCont = \"insiders\"\r\n",
        "\r\n",
        "# define variables for blob store connection\r\n",
        "storage_acct = \"sparkmtndatalake\"\r\n",
        "container_name = \"insiders\"\r\n",
        "linked_svc = \"SparkMtnLake\"\r\n",
        "\r\n",
        "# sas token will be pulled from linked service definition\r\n",
        "sas_token = mssparkutils.credentials.getConnectionStringOrCreds( linked_svc )\r\n",
        "\r\n",
        "spark.conf.set( \"fs.azure.sas.%s.%s.dfs.core.usgovcloudapi.net\" % ( container_name, storage_acct), sas_token )\r\n",
        "\r\n",
        "httpsUrl = \"https://%s.x.y.z.abc/\" % ( storage_acct )\r\n",
        "abfssUrl = \"abfss://%s@%s.x.y.z.abc/\" % ( container_name, storage_acct )\r\n",
        "\r\n",
        "rawFldr = \"/raw/\"\r\n",
        "bronzeFldr =  \"/bronze/\"\r\n",
        "silverFldr = \"/silver/\"\r\n",
        "\r\n",
        "blobSvcConn = BlobServiceClient( httpsUrl, credential = sas_token )\r\n",
        "\r\n",
        "contClient = blobSvcConn.get_container_client( container_name )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##############################\r\n",
        "# bronze layer steps, part 1 #\r\n",
        "##############################\r\n",
        "\r\n",
        "obsFileList = []\r\n",
        "claimFileList = []\r\n",
        "\r\n",
        "for item in contClient.walk_blobs( rawFldr ) :\r\n",
        "\r\n",
        "    # print( item.name )\r\n",
        "\r\n",
        "    for subItem in contClient.walk_blobs( item.name ) :\r\n",
        "\r\n",
        "        subItemName = subItem.name\r\n",
        "\r\n",
        "        # print( subItemName )\r\n",
        "\r\n",
        "        subItemSplit = subItemName.split( \"/\")\r\n",
        "\r\n",
        "        # print( subItemSplit )\r\n",
        "\r\n",
        "        fileName = subItemSplit[ -1 ]\r\n",
        "\r\n",
        "        # print( fileName )\r\n",
        "\r\n",
        "        ######################################\r\n",
        "        # copy patient files to bronze layer #\r\n",
        "        ######################################\r\n",
        "\r\n",
        "        if fileName.upper() == \"PATIENT.NDJSON\" :\r\n",
        "\r\n",
        "            # srcBlob = blobSvcConn.get_blob_client( container_name, subItemName )\r\n",
        "            srcBlob = httpsUrl + container_name + \"/\" + subItemName + sas_token\r\n",
        "            # print( srcBlob )\r\n",
        "\r\n",
        "            newSIName = subItemName.replace( \"raw/\", \"bronze/reference_data/\" )\r\n",
        "\r\n",
        "            print( \"Copying \", srcBlob, \" to \", newSIName )\r\n",
        "\r\n",
        "            destBlob = blobSvcConn.get_blob_client( container_name, newSIName )\r\n",
        "\r\n",
        "            destBlob.start_copy_from_url( srcBlob )\r\n",
        "\r\n",
        "            # check status of copy\r\n",
        "            status = None\r\n",
        "\r\n",
        "            for i in range( 100 ) :\r\n",
        "\r\n",
        "                blobProps = destBlob.get_blob_properties()\r\n",
        "\r\n",
        "                status = blobProps.copy.status\r\n",
        "\r\n",
        "                print( \"Copy Status: \", status )\r\n",
        "\r\n",
        "                if status == \"success\" :\r\n",
        "\r\n",
        "                    break\r\n",
        "\r\n",
        "                time.sleep( 10 ) # ten second increments\r\n",
        "\r\n",
        "        elif fileName.upper() == \"OBSERVATION.NDJSON\" :\r\n",
        "\r\n",
        "            obsFileList.append( subItemName )\r\n",
        "\r\n",
        "        elif fileName.upper() == \"CLAIM.NDJSON\" :\r\n",
        "\r\n",
        "            claimFileList.append( subItemName )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "########################\r\n",
        "# bronze layer, part 2 #\r\n",
        "########################\r\n",
        "# get file lists created above\r\n",
        "print( \"*****\" )\r\n",
        "print( \"Partiioning Claims Files by Issued Year :\")\r\n",
        "print( \"*****\" )\r\n",
        "\r\n",
        "# print( claimFileList )\r\n",
        "\r\n",
        "filesToLoadList = []\r\n",
        "\r\n",
        "# cheated a little here\r\n",
        "writeUrlPath = abfssUrl + \"bronze/\"\r\n",
        "firstFilePath = abfssUrl + \"raw/000047ca-00c7-492b-bf65-740805144cd2/\"\r\n",
        "\r\n",
        "# create schema from first Claims file\r\n",
        "pathClaimSchema = firstFilePath + \"Claim.ndjson\"\r\n",
        "claimSchema = spark.read.option( \"multiline\", \"true\" ).json( pathClaimSchema ).schema\r\n",
        "\r\n",
        "for file in claimFileList :\r\n",
        "\r\n",
        "    fileToLoad = abfssUrl + file\r\n",
        "\r\n",
        "    # print( fileToLoad )\r\n",
        "\r\n",
        "    filesToLoadList.append( fileToLoad )\r\n",
        "\r\n",
        "# single Claims dataframe with all Claims files\r\n",
        "claimsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).schema( claimSchema ).json( filesToLoadList )\r\n",
        "# claimsDf.show( 10 )\r\n",
        "\r\n",
        "# historic data write\r\n",
        "exportClaimsDf = claimsDf.withColumn( \"year\", date_format( col( \"created\" ), \"yyyy\" ) ).repartition( \"year\" )\r\n",
        "\r\n",
        "writePath = writeUrlPath + \"historic_data/Claim/\"\r\n",
        "\r\n",
        "# exportClaimsDf.filter( ( exportClaimsDf.year >= 2016 ) & ( exportClaimsDf.year < 2021 ) ).write.partitionBy( \"year\" ).mode( \"overwrite\" ).json( writePath )\r\n",
        "\r\n",
        "# incremental data write\r\n",
        "exportClaimsDf = claimsDf.withColumn( \"year\", date_format( col( \"created\" ), \"yyyy\" ) ).withColumn( \"month\", date_format( col( \"created\" ), \"mm\" ) ).withColumn( \"day\", date_format( col( \"created\" ), \"dd\" ) ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "\r\n",
        "writePath = writeUrlPath + \"incremental_data/Claim/\"\r\n",
        "\r\n",
        "# exportClaimsDf.filter( \"year >= 2021\" ).write.partitionBy( \"year\", \"month\", \"day\" ).mode( \"overwrite\" ).json( writePath )\r\n",
        "\r\n",
        "print( \"*****\" )\r\n",
        "print( \"Partitioning Observations Files by Issued Year :\" )\r\n",
        "print( \"*****\" )\r\n",
        "\r\n",
        "filesToLoadList = []\r\n",
        "\r\n",
        "pathObsSchema = firstFilePath + \"Observation.ndjson\"\r\n",
        "obSchema = spark.read.option( \"multiline\", \"true\" ).json( pathObsSchema ).schema\r\n",
        "\r\n",
        "for file in obsFileList :\r\n",
        "\r\n",
        "    fileToLoad = abfssUrl + file\r\n",
        "\r\n",
        "    filesToLoadList.append( fileToLoad )\r\n",
        "\r\n",
        "# single dataframe for all Observation records\r\n",
        "obsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).schema( obSchema ).json( filesToLoadList )\r\n",
        "\r\n",
        "# historic data write\r\n",
        "exportObsDf = obsDf.withColumn( \"year\", date_format( col( \"issued\" ), \"yyyy\" ) ).repartition( \"year\" )\r\n",
        "\r\n",
        "writePath = writeUrlPath + \"historic_data/Observation/\"\r\n",
        "\r\n",
        "# exportObsDf.filter( ( exportObsDf.year >= 2016 ) & ( exportObsDf.year < 2021 ) ).write.partitionBy( \"year\" ).mode( \"overwrite\" ).json( writePath )\r\n",
        "\r\n",
        "# incremental data write\r\n",
        "exportObsDf = obsDf.withColumn( \"year\", date_format( col( \"issued\" ), \"yyyy\" ) ).withColumn( \"month\", date_format( col( \"issued\" ), \"mm\" ) ).withColumn( \"day\", date_format( col( \"issued\" ), \"dd\" ) ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "\r\n",
        "writePath = writeUrlPath + \"incremental_data/Observation/\"\r\n",
        "\r\n",
        "# exportObsDf.filter( \"year >= 2021\" ).write.partitionBy( \"year\", \"month\", \"day\" ).mode( \"overwrite\" ).json( writePath )"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": false,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
