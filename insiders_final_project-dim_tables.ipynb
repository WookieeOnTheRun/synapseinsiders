{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "# import necessary packages and libraries\r\n",
        "import json, datetime, time\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "import com.microsoft.spark.sqlanalytics\r\n",
        "from com.microsoft.spark.sqlanalytics.Constants import Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define necessary connections to storage ( source and destination )\r\n",
        "# abfss because my storage account has HNS enabled\r\n",
        "adlsAcct = \"sparkmtndatalake.dfs.core.usgovcloudapi.net/\"\r\n",
        "# SAS Token Expiration : 27-April-2023\r\n",
        "adlsSas = \"?sv=2021-12-02&ss=b&srt=sco&sp=rwdlacx&se=2023-04-27T22:50:09Z&st=2023-03-28T14:50:09Z&spr=https&sig=f74oVHLZZ0IyNULznbqdgLeF50aCHn8rnegQdG6VcOs%3D\"\r\n",
        "adlsCont = \"insiders\"\r\n",
        "\r\n",
        "# define variables for blob store connection\r\n",
        "storage_acct = \"sparkmtndatalake\"\r\n",
        "container_name = \"insiders\"\r\n",
        "linked_svc = \"SparkMtnLake\"\r\n",
        "\r\n",
        "# sas token will be pulled from linked service definition\r\n",
        "sas_token = mssparkutils.credentials.getConnectionStringOrCreds( linked_svc )\r\n",
        "\r\n",
        "spark.conf.set( \"fs.azure.sas.%s.%s.dfs.core.usgovcloudapi.net\" % ( container_name, storage_acct), sas_token )\r\n",
        "\r\n",
        "httpsUrl = \"https://%s.blob.core.usgovcloudapi.net/\" % ( storage_acct )\r\n",
        "abfssUrl = \"abfss://%s@%s.dfs.core.usgovcloudapi.net/\" % ( container_name, storage_acct )\r\n",
        "\r\n",
        "rawFldr = \"raw/\"\r\n",
        "\r\n",
        "blobSvcConn = BlobServiceClient( httpsUrl, credential = sas_token )\r\n",
        "\r\n",
        "contClient = blobSvcConn.get_container_client( container_name )\r\n",
        "\r\n",
        "firstFilePath = rawFldr + \"007a4ada-4bfe-4e15-a180-b5844dd2d275/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# files to load for dim tables\r\n",
        "\r\n",
        "filesToLoad = []\r\n",
        "\r\n",
        "for item in contClient.walk_blobs( rawFldr ) :\r\n",
        "\r\n",
        "    # print( item.name )\r\n",
        "\r\n",
        "    for subItem in contClient.walk_blobs( item.name ) :\r\n",
        "\r\n",
        "        # print( subItem.name )\r\n",
        "\r\n",
        "        subItemSplit = ( subItem.name ).split( \"/\" )\r\n",
        "\r\n",
        "        fileName = subItemSplit[ -1 ]\r\n",
        "\r\n",
        "        if fileName.upper() not in filesToLoad and fileName.upper() not in [ \"CLAIM.NDJSON\", \"OBSERVATION.NDJSON\", \"PATIENT.NDJSON\" ] :\r\n",
        "\r\n",
        "            filesToLoad.append( ( subItem.name ).upper() )\r\n",
        "\r\n",
        "print( filesToLoad )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# test two files with naming convention : \r\n",
        "# hospitalInformation\r\n",
        "# practitionerInformation\r\n",
        "\r\n",
        "fileToDfName = {}\r\n",
        "\r\n",
        "firstFileList = []\r\n",
        "\r\n",
        "for item in contClient.walk_blobs( firstFilePath ) :\r\n",
        "\r\n",
        "    # print( item.name )\r\n",
        "\r\n",
        "    itemFullName = item.name\r\n",
        "\r\n",
        "    schemaPath = abfssUrl + itemFullName\r\n",
        "\r\n",
        "    itemSplit = itemFullName.split( \"/\" )\r\n",
        "\r\n",
        "    fileName = itemSplit[ -1 ]\r\n",
        "\r\n",
        "    fileSplit = fileName.split( \".\" )\r\n",
        "\r\n",
        "    if fileName not in fileToDfName.keys() :\r\n",
        "\r\n",
        "        if \"hospitalInformation\" in fileName :\r\n",
        "\r\n",
        "            dfName = \"df_hospitalInformation\"\r\n",
        "            schemaName = \"sch_hospitalInformation\"\r\n",
        "\r\n",
        "        elif \"practitionerInformation\" in fileName :\r\n",
        "\r\n",
        "            dfName = \"df_practitionerInformation\"\r\n",
        "            schemaName = \"sch_practitionerInformation\"\r\n",
        "\r\n",
        "        else :\r\n",
        "\r\n",
        "            dfName = \"df_\" + fileSplit[ 0 ]\r\n",
        "            schemaName = \"sch_\" + fileSplit[ 0 ]\r\n",
        "\r\n",
        "        itemsList = []\r\n",
        "        itemsList.append( dfName )\r\n",
        "        itemsList.append( schemaName )\r\n",
        "\r\n",
        "        fileToDfName[ fileName ] = itemsList\r\n",
        "\r\n",
        "print( fileToDfName )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# spark.read.option( \"multiline\", \"true\" ).json( pathClaimSchema ).schema\r\n",
        "# spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).schema( claimSchema ).json( filesToLoadList )\r\n",
        "\r\n",
        "for file in fileToDfName.keys() :\r\n",
        "\r\n",
        "    schemaPath = abfssUrl + firstFilePath + file\r\n",
        "\r\n",
        "    dfName = fileToDfName[ file ][ 0 ]\r\n",
        "    schName = fileToDfName[ file ][ 1 ]\r\n",
        "\r\n",
        "    dynCmdStr = schName + \" = spark.read.option( 'multiline', 'true' ).json( '\" + schemaPath + \"' ).schema\"\r\n",
        "\r\n",
        "    print( dynCmdStr )\r\n",
        "    exec( dynCmdStr )\r\n",
        "\r\n",
        "    if 'hospitalInformation' in file :\r\n",
        "\r\n",
        "        hiFileList = []\r\n",
        "\r\n",
        "        for fileToLoad in filesToLoad :\r\n",
        "\r\n",
        "            if 'hospitalInformation' in fileToLoad :\r\n",
        "\r\n",
        "                fileString = abfssUrl + fileToLoad\r\n",
        "\r\n",
        "                hiFileList.append( fileString )\r\n",
        "\r\n",
        "        dynCmdStr = dfName + \" = spark.read.option( 'multiline', 'true' ).option( 'columnNameOfCorruptRecord', 'corruptRecord' ).schema( \" + schName + \" ).json( hiFileList )\"\r\n",
        "\r\n",
        "        print( dynCmdStr )\r\n",
        "        exec( dynCmdStr )\r\n",
        "\r\n",
        "    elif 'practitionerInformation' in file :\r\n",
        "\r\n",
        "        piFileList = []\r\n",
        "\r\n",
        "        for fileToLoad in filesToLoad :\r\n",
        "\r\n",
        "            if 'practitionerInformation' in fileToLoad :\r\n",
        "\r\n",
        "                fileString = abfssUrl + fileToLoad\r\n",
        "\r\n",
        "                piFileList.append( fileString )\r\n",
        "\r\n",
        "        dynCmdStr = dfName + \" = spark.read.option( 'multiline', 'true' ).option( 'columnNameOfCorruptRecord', 'corruptRecord' ).schema( \" + schName + \" ).json( piFileList )\"\r\n",
        "\r\n",
        "        print( dynCmdStr )\r\n",
        "        exec( dynCmdStr )\r\n",
        "\r\n",
        "    else :\r\n",
        "\r\n",
        "        filePath = abfssUrl + rawFldr + \"*/\" + file\r\n",
        "\r\n",
        "        dynCmdStr = dfName + \" = spark.read.option( 'multiline', 'true' ).option( 'columnNameOfCorruptRecord', 'corruptRecord' ).schema( \" + schName + \" ).json( '\" + filePath + \"' )\"\r\n",
        "\r\n",
        "        print( dynCmdStr )\r\n",
        "        exec( dynCmdStr )\r\n",
        "\r\n",
        "    # dynamic cmd to write dataframe contents to dedicated pool table in 'dim' schema\r\n",
        "\r\n",
        "    dfSplit = dfName.split( \"_\" )\r\n",
        "\r\n",
        "    tblName = dfSplit[ -1 ]\r\n",
        "\r\n",
        "    dynCmdStr = \"( \" + dfName + \".write.option( Constants.SERVER, '<dedicated pool instance url>' ).option( Constants.USER, 'sqladminuser' ).option( Constants.PASSWORD, '<sqladminuser_pwd>' ).option( Constants.STAGING_STORAGE_ACCOUNT_KEY, '<storage account key>' ).option( Constants.TEMP_FOLDER, '<abfss url for storage account, container and parquet file write folder>' ).mode( 'overwrite' ).synapsesql( '<sqldb>.dim.\" + tblName + \"' ) )\"\r\n",
        "    print( dynCmdStr )\r\n",
        "    # exec( dynCmdStr )"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": false,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}