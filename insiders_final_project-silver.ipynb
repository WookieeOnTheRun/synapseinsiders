{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "# import necessary packages and libraries\r\n",
        "import json, datetime, time\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient\r\n",
        "\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "from delta import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define necessary connections to storage ( source and destination )\r\n",
        "# abfss because my storage account has HNS enabled\r\n",
        "adlsAcct = \"<storage account url>\"\r\n",
        "adlsSas = \"<sas token>\"\r\n",
        "adlsCont = \"insiders\"\r\n",
        "\r\n",
        "# define variables for blob store connection\r\n",
        "storage_acct = \"sparkmtndatalake\"\r\n",
        "container_name = \"insiders\"\r\n",
        "linked_svc = \"SparkMtnLake\"\r\n",
        "\r\n",
        "# sas token will be pulled from linked service definition\r\n",
        "sas_token = mssparkutils.credentials.getConnectionStringOrCreds( linked_svc )\r\n",
        "\r\n",
        "spark.conf.set( \"fs.az.sas.%s.%s.x.y.z.abc\" % ( container_name, storage_acct), sas_token )\r\n",
        "\r\n",
        "httpsUrl = \"https://%s.x.y.z.abc/\" % ( storage_acct )\r\n",
        "abfssUrl = \"abfss://%s@%s.x.y.z.abc/\" % ( container_name, storage_acct )\r\n",
        "\r\n",
        "rawFldr = \"/raw/\"\r\n",
        "bronzeFldr =  \"/bronze/\"\r\n",
        "silverFldr = \"/silver/\"\r\n",
        "\r\n",
        "blobSvcConn = BlobServiceClient( httpsUrl, credential = sas_token )\r\n",
        "\r\n",
        "contClient = blobSvcConn.get_container_client( container_name )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# silver layer\r\n",
        "# part 1 - patient data\r\n",
        "\r\n",
        "# chreate schema first to do initial load\r\n",
        "# grab first file manually - keep it simple\r\n",
        "firstFilePath = abfssUrl + \"bronze/reference_data/000047ca-00c7-492b-bf65-740805144cd2/\"\r\n",
        "\r\n",
        "# create schema from first Patient file\r\n",
        "pathPatientSchema = firstFilePath + \"Patient.ndjson\"\r\n",
        "patientSchema = spark.read.option( \"multiline\", \"true\" ).json( pathPatientSchema ).schema\r\n",
        "\r\n",
        "abfssFileList = abfssUrl + \"bronze/reference_data/*/Patient.ndjson\"\r\n",
        "\r\n",
        "silverExportPath = abfssUrl + \"silver/\"\r\n",
        "\r\n",
        "# recursively load all Patient files into new DF with schema defined above\r\n",
        "patientDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).option( \"recursiveFileLookup\", \"true\" ).schema( patientSchema ).json( abfssFileList )\r\n",
        "# patientDf.show( 10, False )\r\n",
        "# patientDf.printSchema()\r\n",
        "\r\n",
        "##################\r\n",
        "# patientAddress #\r\n",
        "##################\r\n",
        "# \"deceasedDateTime\", - between gender and martialStatus\r\n",
        "patientAddressDf = patientDf.select( \"id\", \"name\", \"birthDate\", \"gender\", explode_outer( \"address\" ).alias( \"addressExpand\") ).select( \"id\", \"name\", \"birthDate\", \"gender\", \"addressExpand\" )\r\n",
        "\r\n",
        "# patientAddressDf.printSchema()\r\n",
        "# patientAddressDf.show( 10, False )\r\n",
        "\r\n",
        "# export to delta\r\n",
        "exportPath = silverExportPath + \"patientAddress/\"\r\n",
        "# patientAddressDf.select( \"id\", \"birthDate\", \"gender\", \"addressExpand.city\", \"addressExpand.country\", \"addressExpand.postalCode\", \"addressExpand.state\" ).select( \"id\", \"birthDate\", \"gender\", \"city\", \"country\", \"postalCode\", \"state\" ).write.format( \"delta\" ).save( exportPath )\r\n",
        "\r\n",
        "#####################\r\n",
        "# patientIdentifier #\r\n",
        "#####################\r\n",
        "patientIdDf = patientDf.select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", explode_outer( \"name\" ).alias( \"nameExpand\" ) ).select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", \"nameExpand\" )\r\n",
        "\r\n",
        "# patientIdDf.printSchema()\r\n",
        "# patientIdDf.show( 10, False )\r\n",
        "\r\n",
        "# export to delta\r\n",
        "exportPath = silverExportPath + \"patientIdentification/\"\r\n",
        "# patientIdDf.select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", \"nameExpand.family\", \"nameExpand.given\" ).select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", \"family\", \"given\" ).write.format( \"delta\" ).save( exportPath )\r\n",
        "\r\n",
        "####################\r\n",
        "# patientExtension #\r\n",
        "####################\r\n",
        "patientExtensionDf = patientDf.select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\",  explode_outer( \"extension\" ).alias( \"extensionExpand\" ) ).select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\",  \"extensionExpand\" )\r\n",
        "\r\n",
        "# patientExtensionDf.printSchema()\r\n",
        "# patientExtensionDf.show( 10, False )\r\n",
        "\r\n",
        "# export to delta\r\n",
        "# patientExtensionDf.show( 10 )\r\n",
        "exportPath = silverExportPath + \"patientExtension/\"\r\n",
        "# patientExtensionDf.select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", \"extensionExpand.url\", \"extensionExpand.valueAddress\", \"extensionExpand.valueDecimal\", \"extensionExpand.valueString\" ).select( \"id\", \"birthDate\", \"gender\", \"maritalStatus\", \"url\", \"valueAddress\", \"valueDecimal\", \"valueString\" ).write.format( \"delta\" ).save( exportPath )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# silver layer\r\n",
        "# part 2 - Claims data\r\n",
        "\r\n",
        "# https://sparkmtndatalake.blob.core.usgovcloudapi.net/insiders/bronze/historic_data/Claim/year=2016/part-00000-af12a187-6fd9-4b3f-9d52-afe965b9f9dd.c000.json\r\n",
        "\r\n",
        "silverExportPath = abfssUrl + \"silver/\"\r\n",
        "historicalFilePath = abfssUrl + \"bronze/historic_data/\"\r\n",
        "incrementalFilePath = abfssUrl + \"bronze/incremental_data/\"\r\n",
        "\r\n",
        "# load claims files\r\n",
        "histFirstFilePath = historicalFilePath + \"Claim/year=2016/part-00000-af12a187-6fd9-4b3f-9d52-afe965b9f9dd.c000.json\"\r\n",
        "incrFirstFilePath = incrementalFilePath + \"Claim/year=2021/month=02/day=08/part-00000-efc1c61f-2cdd-43ee-b7f7-2c96f6d5d7a3.c000.json\"\r\n",
        "\r\n",
        "## historical ##\r\n",
        "# create schema first to do initial file load\r\n",
        "pathClaimSchema = histFirstFilePath\r\n",
        "claimSchema = spark.read.option( \"multiline\", \"true\" ).json( pathClaimSchema ).schema\r\n",
        "\r\n",
        "abfssFileList = historicalFilePath + \"Claim/*/\"\r\n",
        "\r\n",
        "# recursively load historic Claims files\r\n",
        "historyClaimsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).option( \"recursiveFileLookup\", \"true\" ).schema( claimSchema ).json( abfssFileList )\r\n",
        "# historyClaimsDf.printSchema()\r\n",
        "# historyClaimsDf.show( 10, False )\r\n",
        "\r\n",
        "histClaimsDf = historyClaimsDf.withColumn( \"createdDate\", to_date( col( \"created\" ) ).cast( \"date\" ) )\r\n",
        "# histClaimsDf.printSchema()\r\n",
        "# histClaimsDf.show( 10, False )\r\n",
        "\r\n",
        "## incremental ##\r\n",
        "# create schema\r\n",
        "pathClaimSchema = incrFirstFilePath\r\n",
        "claimSchema = spark.read.option( \"multiline\", \"true\" ).json( pathClaimSchema ).schema\r\n",
        "\r\n",
        "abfssFileList = incrementalFilePath + \"Claim/\"\r\n",
        "\r\n",
        "# recursively load incremental Claims files\r\n",
        "incrementClaimsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).option( \"recursiveFileLookup\", \"true\" ).schema( claimSchema ).json( abfssFileList )\r\n",
        "# incrementClaimsDf.printSchema()\r\n",
        "# incrementClaimsDf.show( 10, False )\r\n",
        "\r\n",
        "incrClaimsDf = incrementClaimsDf.withColumn( \"createdDate\", to_date( col( \"created\" ) ).cast( \"date\" ) )\r\n",
        "# incrClaimsDf.printSchema()\r\n",
        "# incrClaimsDf.show( 10, False )\r\n",
        "\r\n",
        "####################\r\n",
        "# claimInsurance #\r\n",
        "####################\r\n",
        "exportPath = silverExportPath + \"claimInsurance/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "claimInsuranceDf = histClaimsDf.select( \"billablePeriod\", \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), explode_outer( \"insurance\" ).alias( \"insuranceExpand\" ), \"createdDate\" ).select( \"billablePeriod\", \"created\", \"id\", \"patientId\", \"insuranceExpand\", \"createdDate\" )\r\n",
        "exportClaimsInsDf = claimInsuranceDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).repartition( \"year\" )\r\n",
        "# exportClaimsInsDf.printSchema()\r\n",
        "\r\n",
        "# export historical claimInsuranceDf\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# exportClaimsInsDf.select( \"billablePeriod\", \"created\", \"year\", \"id\", \"patientId\", \"insuranceExpand.coverage\" ).select( \"billablePeriod\", \"created\", \"id\", \"patientId\", \"coverage\", \"year\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "claimInsuranceDf = incrClaimsDf.select( \"billablePeriod\", \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), explode_outer( \"insurance\" ).alias( \"insuranceExpand\" ), \"createdDate\" ).select( \"billablePeriod\", \"created\", \"id\", \"patientId\", \"insuranceExpand\", \"createdDate\" )\r\n",
        "# claimInsuranceDf.printSchema()\r\n",
        "exportClaimsInsDf = claimInsuranceDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).withColumn( \"month\", month( col( \"createdDate\" ) ) ).withColumn( \"day\", date_format( col( \"createdDate\" ), \"d\" ) ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "\r\n",
        "# export incremental claims\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# exportClaimsInsDf.select( \"billablePeriod\", \"created\", \"year\", \"month\", \"day\", \"id\", \"patientId\", \"insuranceExpand.coverage\" ).select( \"billablePeriod\", \"created\", \"id\", \"patientId\", \"coverage\", \"year\", \"month\", \"day\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )\r\n",
        "\r\n",
        "##################\r\n",
        "# claimDiagnosis #\r\n",
        "##################\r\n",
        "exportPath = silverExportPath + \"claimDiagnosis/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "claimDiagDf = histClaimsDf.select( \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), col( \"patient.display\" ).alias( \"patientFirstAndLast\" ), col( \"priority.coding\" ).alias( \"priorityCoding\" ), col( \"provider.display\" ).alias( \"providerDisplay\" ), \"resourceType\", \"status\", col( \"total.value\" ).alias( \"totalValue\" ), explode_outer( \"item\").alias( \"itemExpand\" ), \"createdDate\" ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCoding\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"itemExpand\", \"createdDate\" )\r\n",
        "\r\n",
        "# claimDiagDf.printSchema()\r\n",
        "# claimDiagDf.show( 10, False )\r\n",
        "\r\n",
        "exportClaimDiagDf = claimDiagDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", col( \"priorityCoding.code\").alias( \"priorityCode\" ), \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", explode_outer( \"itemExpand.productOrService.coding\" ).alias( \"patientProductOrService\" ), \"year\" ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\" ).repartition( \"year\" )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# exportClaimDiagDf.select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "claimDiagDf = incrClaimsDf.select( \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), col( \"patient.display\" ).alias( \"patientFirstAndLast\" ), col( \"priority.coding\" ).alias( \"priorityCoding\" ), col( \"provider.display\" ).alias( \"providerDisplay\" ), \"resourceType\", \"status\", col( \"total.value\" ).alias( \"totalValue\" ), explode_outer( \"item\").alias( \"itemExpand\" ), \"createdDate\" ) .select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCoding\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"itemExpand\", \"createdDate\" )\r\n",
        "\r\n",
        "exportClaimDiagDf = claimDiagDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).withColumn( \"month\", month( col( \"createdDate\" ) ) ).withColumn( \"day\", date_format( col( \"createdDate\" ), \"d\" ) ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", col( \"priorityCoding.code\").alias( \"priorityCode\" ), \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", explode_outer( \"itemExpand.productOrService.coding\" ).alias( \"patientProductOrService\" ), \"year\", \"month\", \"day\" ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\", \"month\", \"day\" ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# exportClaimDiagDf.select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\", \"month\", \"day\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )\r\n",
        "\r\n",
        "##################\r\n",
        "# claimProcedure #\r\n",
        "##################\r\n",
        "\r\n",
        "exportPath = silverExportPath + \"claimProcedure/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "claimProcDf = histClaimsDf.select( \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), col( \"patient.display\" ).alias( \"patientFirstAndLast\" ), col( \"priority.coding\" ).alias( \"priorityCoding\" ), col( \"provider.display\" ).alias( \"providerDisplay\" ), \"resourceType\", \"status\", col( \"total.value\" ).alias( \"totalValue\" ), explode_outer( \"item\").alias( \"itemExpand\" ), \"createdDate\" ) .select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCoding\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"itemExpand\", \"createdDate\" )\r\n",
        "\r\n",
        "# claimDiagDf.printSchema()\r\n",
        "# claimDiagDf.show( 10, False )\r\n",
        "\r\n",
        "exportClaimProcDf = claimProcDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", col( \"priorityCoding.code\").alias( \"priorityCode\" ), \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", explode_outer( \"itemExpand.productOrService.coding\" ).alias( \"patientProductOrService\" ), \"year\" ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\" ).repartition( \"year\" )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# exportClaimProcDf.select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "claimProcDf = incrClaimsDf.select( \"created\", \"id\", col( \"patient.reference\" ).alias( \"patientId\" ), col( \"patient.display\" ).alias( \"patientFirstAndLast\" ), col( \"priority.coding\" ).alias( \"priorityCoding\" ), col( \"provider.display\" ).alias( \"providerDisplay\" ), \"resourceType\", \"status\", col( \"total.value\" ).alias( \"totalValue\" ), explode_outer( \"item\").alias( \"itemExpand\" ), \"createdDate\" ) .select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCoding\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"itemExpand\", \"createdDate\" )\r\n",
        "\r\n",
        "exportClaimProcDf = claimProcDf.withColumn( \"year\", year( col( \"createdDate\" ) ) ).withColumn( \"month\", month( col( \"createdDate\" ) ) ).withColumn( \"day\", date_format( col( \"createdDate\" ), \"d\" ) ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", col( \"priorityCoding.code\").alias( \"priorityCode\" ), \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", explode_outer( \"itemExpand.productOrService.coding\" ).alias( \"patientProductOrService\" ), \"year\", \"month\", \"day\" ).select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\", \"month\", \"day\" ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# exportClaimProcDf.select( \"created\", \"id\", \"patientId\", \"patientFirstAndLast\", \"priorityCode\", \"providerDisplay\", \"resourceType\", \"status\", \"totalValue\", \"patientProductOrService\", \"year\", \"month\", \"day\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# silver layer\r\n",
        "# part 3 - Observations\r\n",
        "\r\n",
        "silverExportPath = abfssUrl + \"silver/\"\r\n",
        "historicalFilePath = abfssUrl + \"bronze/historic_data/\"\r\n",
        "incrementalFilePath = abfssUrl + \"bronze/incremental_data/\"\r\n",
        "\r\n",
        "# load observations files\r\n",
        "histFirstFilePath = historicalFilePath + \"Observation/year=2016/part-00000-4ede0f25-40a2-488b-8796-917f45de2ea7.c000.json\"\r\n",
        "incrFirstFilePath = incrementalFilePath + \"Observation/year=2021/month=02/day=22/part-00000-3420bf8a-7e08-4a6e-84ee-00d1625d2995.c000.json\"\r\n",
        "\r\n",
        "## historical ##\r\n",
        "# create schema first to do initial file load\r\n",
        "pathObsSchema = histFirstFilePath\r\n",
        "obsSchema = spark.read.option( \"multiline\", \"true\" ).json( pathObsSchema ).schema\r\n",
        "\r\n",
        "abfssFileList = historicalFilePath + \"Observation/\"\r\n",
        "\r\n",
        "# recursively load historic Claims files\r\n",
        "historyObsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).option( \"recursiveFileLookup\", \"true\" ).schema( obsSchema ).json( abfssFileList )\r\n",
        "# historyObsDf.printSchema()\r\n",
        "# historyObsDf.show( 10, False )\r\n",
        "\r\n",
        "histObsDf = historyObsDf.withColumn( \"issuedDate\", to_date( col( \"issued\" ) ).cast( \"date\" ) )\r\n",
        "# histObsDf.printSchema()\r\n",
        "# histObsDf.show( 10, False )\r\n",
        "\r\n",
        "## incremental ##\r\n",
        "# create schema\r\n",
        "pathObsSchema = incrFirstFilePath\r\n",
        "obsSchema = spark.read.option( \"multiline\", \"true\" ).json( pathObsSchema ).schema\r\n",
        "\r\n",
        "abfssFileList = incrementalFilePath + \"Observation/\"\r\n",
        "\r\n",
        "# recursively load incremental Claims files\r\n",
        "incrementObsDf = spark.read.option( \"multiline\", \"true\" ).option( \"columnNameOfCorruptRecord\", \"corruptRecord\" ).option( \"recursiveFileLookup\", \"true\" ).schema( obsSchema ).json( abfssFileList )\r\n",
        "# incrementObsDf.printSchema()\r\n",
        "# incrementObsDf.show( 10, False )\r\n",
        "\r\n",
        "incrObsDf = incrementObsDf.withColumn( \"issuedDate\", to_date( col( \"issued\" ) ).cast( \"date\" ) )\r\n",
        "# incrObsDf.printSchema()\r\n",
        "# incrObsDf.show( 10, False )\r\n",
        "\r\n",
        "########################\r\n",
        "# Observation Category #\r\n",
        "########################\r\n",
        "\r\n",
        "exportPath = silverExportPath + \"observationCategory/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "obsCategoryDf = histObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", explode_outer( \"category\" ).alias( \"categoryExpand\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryExpand\" ).repartition( \"year\" )\r\n",
        "# obsCategoryDf.printSchema()\r\n",
        "# obsCategoryDf.show( 10, False )\r\n",
        "\r\n",
        "exportObsCatDf = obsCategoryDf.select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", explode_outer( \"categoryExpand.coding\" ).alias( \"categoryCoding\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", col( \"categoryCoding.display\" ).alias( \"categoryCodeDisplay\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryCodeDisplay\" )\r\n",
        "# exportObsCatDf.printSchema()\r\n",
        "# exportObsCatDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# exportObsCatDf.select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryCodeDisplay\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "obsCategoryDf = incrObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).withColumn( \"month\", month( col( \"issuedDate\" ) ) ).withColumn( \"day\", date_format( col( \"issuedDate\" ), \"d\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", explode_outer( \"category\" ).alias( \"categoryExpand\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryExpand\" ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "# obsCategoryDf.printSchema()\r\n",
        "# obsCategoryDf.show( 10, False )\r\n",
        "\r\n",
        "exportObsCatDf = obsCategoryDf.select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", explode_outer( \"categoryExpand.coding\" ).alias( \"categoryCoding\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", col( \"categoryCoding.display\" ).alias( \"categoryCodeDisplay\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryCodeDisplay\" )\r\n",
        "# exportObsCatDf.printSchema()\r\n",
        "# exportObsCatDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# exportObsCatDf.select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"categoryCodeDisplay\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )\r\n",
        "\r\n",
        "######################\r\n",
        "# Observation Coding #\r\n",
        "######################\r\n",
        "\r\n",
        "exportPath = silverExportPath + \"observationCoding/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "obsCodingDf = histObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", explode_outer( \"code.coding\" ).alias( \"codingExpand\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"codingExpand\" ).repartition( \"year\" )\r\n",
        "# obsCodingDf.printSchema()\r\n",
        "# obsCodingDf.show( 10, False )\r\n",
        "\r\n",
        "exportObsCodingDf = obsCodingDf.select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", col( \"codingExpand.code\" ).alias( \"code\"), col( \"codingExpand.display\" ).alias( \"codeDisplay\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"code\", \"codeDisplay\" )\r\n",
        "# exportObsCodingDf.printSchema()\r\n",
        "# exportObsCodingDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# exportObsCodingDf.select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"code\", \"codeDisplay\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "obsCodingDf = incrObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).withColumn( \"month\", month( col( \"issuedDate\" ) ) ).withColumn( \"day\", date_format( col( \"issuedDate\" ), \"d\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", explode_outer( \"code.coding\" ).alias( \"codingExpand\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"codingExpand\" ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "# obsCodingDf.printSchema()\r\n",
        "# obsCodingDf.show( 10, False )\r\n",
        "\r\n",
        "exportObsCodingDf = obsCodingDf.select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", col( \"codingExpand.code\" ).alias( \"code\"), col( \"codingExpand.display\" ).alias( \"codeDisplay\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"code\", \"codeDisplay\" )\r\n",
        "# exportObsCodingDf.printSchema()\r\n",
        "# exportObsCodingDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# exportObsCodingDf.select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"code\", \"codeDisplay\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )\r\n",
        "\r\n",
        "#############################\r\n",
        "# Observation valueQuantity #\r\n",
        "#############################\r\n",
        "\r\n",
        "exportPath = silverExportPath + \"observationValueQuantity/\"\r\n",
        "\r\n",
        "# historical\r\n",
        "obsVqDf = histObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", col( \"valueQuantity.unit\" ).alias( \"measureUnit\"), col( \"valueQuantity.value\" ).alias( \"measureValue\" ) ).select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"measureUnit\", \"measureValue\" ).repartition( \"year\" )\r\n",
        "# obsVqDf.printSchema()\r\n",
        "# obsVqDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"historical/\"\r\n",
        "# obsVqDf.select( \"year\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"measureUnit\", \"measureValue\" ).write.format( \"delta\" ).partitionBy( \"year\" ).save( localExportPath )\r\n",
        "\r\n",
        "# incremental\r\n",
        "obsVqDf = incrObsDf.withColumn( \"year\", year( col( \"issuedDate\" ) ) ).withColumn( \"month\", month( col( \"issuedDate\" ) ) ).withColumn( \"day\", date_format( col( \"issuedDate\" ), \"d\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"subject.reference\", col( \"valueQuantity.unit\" ).alias( \"measureUnit\"), col( \"valueQuantity.value\" ).alias( \"measureValue\" ) ).select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"measureUnit\", \"measureValue\" ).repartition( \"year\", \"month\", \"day\" )\r\n",
        "# obsVqDf.printSchema()\r\n",
        "# obsVqDf.show( 10, False )\r\n",
        "\r\n",
        "localExportPath = exportPath + \"incremental/\"\r\n",
        "# obsVqDf.select( \"year\", \"month\", \"day\", \"effectiveDateTime\", \"id\", \"issued\", \"resourceType\", \"status\", \"reference\", \"measureUnit\", \"measureValue\" ).write.format( \"delta\" ).partitionBy( \"year\", \"month\", \"day\" ).save( localExportPath )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "obsVqDf.printSchema()\r\n",
        "obsVqDf.show( 10, False )"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": false,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}
